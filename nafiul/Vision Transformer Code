pip install tensorflow tensorflow-addons
from tensorflow import lite
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
import random, os
import shutil
import matplotlib.pyplot as plt
from matplotlib.image import imread
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.metrics import categorical_accuracy
from sklearn.model_selection import train_test_split
import kagglehub

# Download latest version
path = kagglehub.dataset_download("sovitrath/diabetic-retinopathy-224x224-2019-data")

print("Path to dataset files:", path)
# Add an additional column, mapping to the type
df = pd.read_csv(r'../root/.cache/kagglehub/datasets/sovitrath/diabetic-retinopathy-224x224-2019-data/versions/4/train.csv')

diagnosis_dict_binary = {
    0: 'No_DR',
    1: 'DR',
    2: 'DR',
    3: 'DR',
    4: 'DR'
}

diagnosis_dict = {
    0: 'No_DR',
    1: 'Mild',
    2: 'Moderate',
    3: 'Severe',
    4: 'Proliferate_DR',
}


df['binary_type'] =  df['diagnosis'].map(diagnosis_dict_binary.get)
df['type'] = df['diagnosis'].map(diagnosis_dict.get)
#extract training, validation and train data from the database (Training 72.25%, validation 15%, test 1.25%)
train_intermediate, val = train_test_split(df, test_size = 0.15, stratify = df['type'])
train, test = train_test_split(train_intermediate, test_size = 0.15 / (1 - 0.15), stratify = train_intermediate['type'])
# Create directories for train, val and test. update it everytime the code runs with shutil.rmtree
base_dir = '.\dataset'

train_dir = os.path.join(base_dir, 'train')
val_dir = os.path.join(base_dir, 'val')
test_dir = os.path.join(base_dir, 'test')

if os.path.exists(base_dir):
    shutil.rmtree(base_dir)

# Make directories for train, val and test
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

valid_types = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferate_DR']
df = df[df['type'].isin(valid_types)]
assert all(train['type'].isin(valid_types))
assert all(val['type'].isin(valid_types))
assert all(test['type'].isin(valid_types))
# Copy train, val and test to their respective directories
src_dir = r'../root/.cache/kagglehub/datasets/sovitrath/diabetic-retinopathy-224x224-2019-data/versions/4/colored_images'
for index, row in train.iterrows():
    diagnosis = row['type']
    binary_diagnosis = row['binary_type']
    id_code = row['id_code'] + ".png"
    srcfile = os.path.join(src_dir, diagnosis, id_code)
    dstfile = os.path.join(train_dir, diagnosis)
    os.makedirs(dstfile, exist_ok = True)
    if os.path.exists(srcfile):
      shutil.copy(srcfile, dstfile)
for index, row in val.iterrows():
    diagnosis = row['type']
    binary_diagnosis = row['binary_type']
    id_code = row['id_code'] + ".png"
    srcfile = os.path.join(src_dir, diagnosis, id_code)
    dstfile = os.path.join(val_dir, diagnosis)
    os.makedirs(dstfile, exist_ok=True)
    if os.path.exists(srcfile):
        shutil.copy(srcfile, dstfile)

for index, row in test.iterrows():
    diagnosis = row['type']
    binary_diagnosis = row['binary_type']
    id_code = row['id_code'] + ".png"
    srcfile = os.path.join(src_dir, diagnosis, id_code)
    dstfile = os.path.join(test_dir, diagnosis)
    os.makedirs(dstfile, exist_ok=True)
    if os.path.exists(srcfile):
        shutil.copy(srcfile, dstfile)
for subdir in [train_dir, val_dir, test_dir]:
  print(f"\nContents of {subdir}:")
  for root, dirs, files in os.walk(subdir):
      print(f"{root}: {len(files)} files")
# Image preprocessing
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define image size for ViT
IMG_SIZE = 224

# ImageDataGenerator for preprocessing and augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)

train_batches = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical'
)

val_batches = val_datagen.flow_from_directory(
    val_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical'
)

test_batches = val_datagen.flow_from_directory(
    test_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical'
)
# Transformer block
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)  # Use training here
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)  # Use training here
        return self.layernorm2(out1 + ffn_output)
# Patch encoder with positional info
from tensorflow.keras.layers import Embedding, Dense, Conv2D, Reshape, Dropout, GlobalAveragePooling1D



class PatchEncoder(tf.keras.layers.Layer):
    def __init__(self, num_patches, embed_dim):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection = Dense(embed_dim)
        self.position_embedding = Embedding(input_dim=num_patches, output_dim=embed_dim)

    def call(self, patch, training=None):
      positions = tf.range(start=0, limit=self.num_patches, delta=1)
      return self.projection(patch) + self.position_embedding(positions)
# ViT model for 5 way classification (predicting DR stages)
def build_vit(input_shape=(224, 224, 3), num_classes=5, patch_size=16, num_patches=196):
    inputs = layers.Input(shape=input_shape)

    # Create patches
    patches = layers.Conv2D(filters=64, kernel_size=patch_size, strides=patch_size, padding="valid")(inputs)
    patches = layers.Reshape((num_patches, -1))(patches)

    # Encode patches
    encoded_patches = PatchEncoder(num_patches=num_patches, embed_dim=64)(patches)

    # Add Transformer blocks
    for _ in range(4):
        encoded_patches = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128)(encoded_patches)

    # Classification head
    representation = layers.GlobalAveragePooling1D()(encoded_patches)
    representation = layers.Dropout(0.1)(representation)
    representation = layers.Dense(64, activation="relu")(representation)
    representation = layers.Dropout(0.1)(representation)
    outputs = layers.Dense(num_classes, activation="softmax")(representation)

    return tf.keras.Model(inputs=inputs, outputs=outputs)

# Build and train model
model = build_vit(input_shape=(224, 224, 3), num_classes=5)


model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
history = model.fit(
    train_batches,
    epochs=10,
    validation_data=val_batches
)
# Test accuracy
test_loss, test_accuracy = model.evaluate(test_batches)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# LINK FOR COMPLETE CODE: https://colab.research.google.com/drive/1UD_bybmnndzhi-tvuaqvHNHP5RWIq5zY?usp=sharing
