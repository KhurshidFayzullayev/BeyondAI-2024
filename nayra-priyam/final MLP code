import torch  # BEST ONE UNTIL NOW!!!!
import torch.nn as nn
import torch.nn.init as init
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import time
overall_start_time = time.time()

model = nn.Sequential(
    nn.Linear(784, 196),  # Input layer starting with number of pixels
    nn.ReLU(),           # ReLU Activation
    nn.Linear(196, 49),  # Hidden layer 1
    nn.ReLU(),           # ReLU Activation again
    nn.Linear(49, 10),   # Hidden layer 2
    # removed softmax since crossentropy does its job anyways
)

for layer in model.modules():
    if isinstance(layer, nn.Linear):
        init.kaiming_uniform_(layer.weight, nonlinearity='relu')
        if layer.bias is not None:
            init.zeros_(layer.bias)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  #
])

train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam optimization
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)
loss_fn = nn.CrossEntropyLoss() # loss function

num_epochs = 15
loss_values = []
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for batch_idx, (images, labels) in enumerate(train_loader):
        images = images.view(-1, 784)
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)  # Calculate the average training loss
    loss_values.append(avg_train_loss)
    print(f'Epoch {epoch+1}/{num_epochs} \t\t Training Loss: {avg_train_loss:.6f}')

print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}") # average loss

plt.figure(figsize=(8, 6))        # start plotting
plt.plot(range(1, num_epochs + 1), loss_values, marker='o', linestyle='-', color='b')
plt.title('Loss Graph')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

end_time = time.time()
overall_time = end_time - overall_start_time
print(f"Total execution time: {overall_time:.2f} seconds")
