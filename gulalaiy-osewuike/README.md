![BeyondAI Banner for Research Projects](../BeyondAI_Banner_Research_Projects_2024.png)

# Weight Initialization for MLPs

1. **Motivation:**

In Multi-layer Perceptrons (MLPs), how the weights are initialized is very important in the training process, as well as the overall performance of the MLP. The right initialization can make training faster, avoid problems like vanishing or exploding gradients, and generally lead to better performance. This project looks into how different weight initialization methods impact MLPs' performance. Weâ€™re also interested in how these methods work together with things like activation functions and optimizers. Ultimately, the goal is to figure out which initialization strategies work best for different situations.

2. **The Research Questions we aim to answer include:**

* How do different weight initialization methods affect the training  and overall performance of MLPs?
* How do initialization methods interact with other factors, like activation functions, optimizers and MLP Depth?
* Can we pinpoint the best weight initialization strategies in different scenarios?

5. explaining your method and implementation
6. Briefly mention and discuss your results
7. Draw your conclusions

> The research poster for this project can be found in the [BeyondAI Proceedings 2024](https://thinkingbeyond.education/beyondai_proceedings_2024/).
